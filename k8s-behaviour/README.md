# K8s Behaviour
Despite the initial impression it might give of being fairly straightforward, Kubernetes is a nevertheless a distributed system composed of multiple components that interact with one another to achieve the desired outcomes. In any such system, though, there will inevitably be complex interactions that result in unpredictable but important behaviour. Specifically, as Kubernetes can be thought of as a virtual overlay on top of "real" resources (e.g., nodes represent actual hosts, Pods are an abstraction over a group of containers running in the same Linux namespace), it is important to understand how Kubernetes represents and synchronizes its internal view of the world and how it affects change. Here, we document a number of Kubernetes behaviours and provide resources and instructions for experimenting with them for yourself.

Unfortunately, it is impossible to document, or even discover, all such behaviours. Instead, we encourage you to explore these interactions further on your own, as is done in the other modules of K8sPlayground.

## Kubelet failure, labels, affinities, taints, and tolerations
Kubernetes relies on the kubelet processes running on its hosts to measure and manipulate host resources and processes (e.g., determine resource availability, set up namespaces, start/stop/restart containers). Therefore, an obvious question is: what happens if a kubelet is unhealthy?

First, run `kubectl get nodes` to get the list of nodes and note that their statuses are all 'Ready'. We will use manifests/deployment.yaml to spin up a Deployment that will create a Pod on k8splayground-worker (run `kubectl apply -f deployment.yaml`). We know that the Pod will be on k8splayground-worker because it has an preference for nodes with the 'mock-server' label, which k8splayground-worker does (it also has a toleration for the 'playground_test' taint so in the absence of k8splayground-worker, it can also run on k8splayground-worker2, which has such a taint).

Now, open a shell onto k8splayground-worker with `docker -it k8splayground-worker /bin/bash`. From this shell, run `ps aux | grep containerd-shim` and note the number of containers currently running on the "host" (if you run `kubectl get pods -A -o wide | grep -w k8splayground-worker` outside of the "host" to display all of the Pods running on k8splayground-worker, it should match the number of containers since each Pod in this case only consists of a single container).

Next, make the kubelet non-executable with `chmod -x /usr/bin/kubelet` to prevent it from restarting, find the pid of the kubelet process with `ps aux | grep kubelet`, and kill the kubelet process. After waiting around a minute, check the node again with `kubectl get nodes` and `kubectl describe k8splayground-worker` outside of the "host". You should notice that k8splayground-worker is now a 'NotReady' state and that the node itself has two taints, 'node.kubernetes.io/unreachable:NoExecute' and 'node.kubernetes.io/unreachable:NoSchedule', indicating that Pods should no longer be scheduled or run on the node. Running `kubectl get pods -A -o wide | grep -w k8splayground-worker` again, you'll notice that the Pods that were already scheduled onto and running on the node continue to be in a 'Running' state.

After waiting another 5 minutes, however, running `kubectl get pods -A -o wide | grep -w k8splayground-worker` again should reveal that a number of Pods have now entered a 'Terminating state'. Examining the Pod we spun up earlier with `kubectl describe pod <name of the Pod>`, we can see that it has the following tolerations: 'node.kubernetes.io/not-ready:NoExecute for 300s' and 'node.kubernetes.io/unreachable:NoExecute for 300s'. These tolerations come default on Pods and are the reason why the Pod continued to be in the 'Running' state until 5 minutes had elapsed since the node became unreachable. Running `ps aux | grep containerd-shim` in the shell into k8splayground-worker reveals, however, that the number of containers running on k8splayground-worker has not decreased despite their Pods being marked as 'Terminating' by Kubernetes. If you run `kubectl get pods -o wide`, however, you'll notice that the Deployment, in an effort to maintain the desired number of running replicas, has spun up another Pod on k8splayground-worker2 even though the container from the previous Pod is still running on k8splayground-worker.

This is our first example of Kubernetes being a virtual overlay on top of "real" resources. The Pods were marked as 'Terminating' because more than 5 minutes had elapsed since the nodes were marked as unreachable. However, both Pods and nodes are Kubernetes internal state abstractions, representing groups of containers and hosts, respectively, and, similar to control systems, their state is updated by sensor measurements and deviations from their desired state are corrected by actuators. In this case, the node was marked as unreachable in response to the control plane no longer being able to reach the kubelet on k8splayground-worker. The kubelet is also the actuator in this case - the entity responsible for managing the actual containers that comprise a Pod. However, since the kubelet is not running, despite Kubernetes marking the Pods as 'Terminating', there is nothing on the host to actually stop the containers and so the containers continue running (this was also alluded to in the pod-behaviour module).

To restore the kubelet, run `chmod +x /usr/bin/kubelet` in the k8splayground-worker shell; the kubelet should restart almost immediately afterward. Running `kubectl get nodes` and `kubectl get pods -A -o wide | grep -w k8splayground-worker` and `ps aux | grep containerd-shim` in a shell into k8splayground-worker should show that k8splayground-worker has returned to a 'Ready' state, that the Pods that were in the 'Terminating' state have been deleted, and that the number of containers on k8splayground-worker has also decreased (again, matching the number of Pods running on it).

## Ordering, finalizers, and dangling resources
Kubernetes uses controllers to modify internal state in response to other changes in internal state (e.g. the Deployment controller watches for changes in Deployment and Pod resources and creates/deletes Pods in response to mismatches between the number of desired and running Pods; you can see examples of user-created controllers [here](https://github.com/pliu/k8soperators)). Unfortunately, this could lead to race conditions between controllers acting on the same resources. To address the specific scenario in which a controller wants to react to a resource's deletion prior to its deletion, Kubernetes uses finalizers - a mark placed on the resource by a controller that prevents the resource's deletion until the finalizer has been removed by that same controller, thus ensuring the controller has a chance to act before the resource's deletion.

Unfortunately, finalizers can lead to deadlocks (e.g. the controller fails to remove the finalizer from the resource), leaving resources stuck in a 'Terminating' state indefinitely. Although this can be resolved by editing the resource in question and removing the blocking finalizer, doing so breaks the desired ordering. In the case of deletion, for example, one might expect certain garbage collection activities to be completed before the resource is deleted. We will look at such an example.

First, we will demonstrate normal behaviour using manifests/collected-resources.yaml (`kubectl apply -f collected-resources.yaml`), which creates a namespace and a Pod running in that namespace (you can verify that they exist with `kubectl get namespaces` and `kubectl get pods -n collected-resources`). Run `kubectl delete namespace collected-resources` to delete the namespace. We'd expect that the Pod in the namespace gets deleted along with the namespace, which is what happens after a small delay (the namespace waits for the Pod to be deleted and the Pod waits for its termination grace period to expire; you should verify that the namespace and Pod no longer exist after the command returns).

Next, we use manifests/dangling-resources.yaml (`kubectl apply -f dangling-resources.yaml`) to create the dangling-resources namespace with a Pod running in it, as above (you should verify this). The difference, however, is the presence of a finalizer on the Pod, preventing the Pod from being deleted. Before deleting the namespace, run `docker exec -it k8splayground-worker /bin/bash` to open a shell into the k8splayground-worker "host" and `ps aux | grep containerd-shim` in that shell and note how many containers are running on k8splayground-worker (we know that the Pod will be on this "host" because the other two "hosts" have taints that the Pod does not tolerate), one of which is the container for the Pod in the dangling-resources namespace.

This time, when we try to delete the namespace with `kubectl delete namespace dangling-resources`, the command will hang and never return since the namespace is waiting for the resources within it (i.e. the Pod, which can't be deleted) to be deleted first before being deleted itself. This reveals an ordering: deletion of the namespace triggers a cascading deletion of all resources within that namespace, with the namespace's deletion paused until the resources it have been successfully deleted. Since this command will hang indefinitely, we can kill it with Ctrl+C.

Running `ps aux | grep containerd-shim` over a short period of time in the shell into the k8splayground-worker "host" should reveal that although the container remains running initially, after a short while, it is terminated (the number of containerd-shim processes should drop by 1). From Kubernetes' view, however, the Pod is still in a 'Terminating' state and hasn't been terminated (verify with `kubectl get pods -n dangling-resources`).

Although namespace finalizers serve the same purpose as finalizers on other resources, their implementation is unique. Whereas most resources list finalizers in their metadata field, namespace finalizers are found in their spec field. Additionally, whereas removing a finalizer is as simple as editing the resource and deleting the finalizer from the manifest for most resources, deleting the namespace finalizer is a bit more involved. Therefore, we have provided a script to remove the finalizer from the dangling-resources namespace - run `scripts/dangling-resources.sh`.

After running the script, the dangling-resources namespace will have been deleted (verify this) but the Pod is in a 'Terminating' state (you should verify this with `kubectl get pods -n dangling-resources`). Trying to edit the Pod with `kubectl edit pod dangling-resources -n dangling-resources` (to remove the finalizer, for example) will fail. However, after recreating the namespace with `kubectl create namespace dangling-resources`, editing the Pod to remove the finalizer will succeed, resulting in the Pod being deleted.

## etcd internals
etcd is a distributed key-value store and is the state store used by Kubernetes to store its state. As such, having some understanding of how Kubernetes represents its state in etcd seems advisable. Being a distributed state store, etcd is very complex (one could argue Kubernetes offloads a great deal of complexity to etcd by delegating state management to it) so we won't be going into how etcd works or how to operate it (yet, anyway).

As mentioned in the root README, when creating the cluster, we port-forward localhost port 2379 on the local machine to port 2379 on the Docker control plane "host" where etcd is running. This allows the provided script, scripts/etcdctl.sh, to connect to the etcd backing the k8splayground cluster.

First, run we will use manifests/deployment.yaml from the first section to spin up a Deployment and its associated Pod with `kubectl apply -f deployment.yaml` (verify that it has started with `kubectl get pods`). Next, run `scripts/etcdctl.sh get / --prefix --keys-only` to list all of the keys in etcd that start with /. You'll notice that for the most part, keys are in the form of /registry/\<resource type>/<if namespaced resource, name of the namespace the object is located in>/<object's name> (e.g. the key /registry/deployments/default/deployment represents the Deployment we just created). Open a shell into k8splayground-worker with `docker exec -it k8splayground-worker /bin/bash` and note the number of running containers with `ps aux | grep containerd-shim`.

Now, delete the etcd key corresponding to the Pod we created earlier with `scripts/etcdctl.sh del /registry/pods/default/ --prefix` (this actually deletes all Pods in the default namespace). However, running `kubectl get pods` should show that there is still a Pod running in the default namespace and running `ps aux | grep containerd-shim` in the k8splayground-worker shell should show that the same number of containers are still running. This is because the Deployment detected that the number of Pods it is managing has diverged from the desired number and started a new Pod to reconcile the difference (if you look closely, the Pod name and one of the container names has changed after deleting the etcd key). Likewise, if we delete the Deployment with `scripts/etcdctl.sh del /registry/depoyments/default/deployment`, running `kubectl get pods` and `ps aux | grep containerd-shim` should reveal that the Deployment-managed Pod and its respective container have also been deleted.

Finally, using manifests/dangling-resources.yaml from the previous section (`kubectl apply -f dangling-resources.yaml`), we see that deleting keys directly from etcd bypasses finalizers (`scripts/etcdctl.sh del /registry/namespaces/dangling-resources`).

## Commands
```
List nodes:
kubectl get nodes

Run etcdctl command:
k8s-behaviour/scripts/etcdctl.sh <etcdctl command> (a good first command to run is 'help')
```
